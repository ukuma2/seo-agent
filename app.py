"""
==============================================================================
ADVANCED SEO CONTENT EVALUATOR
==============================================================================
A robust, error-proof Streamlit application that:
1. Crawls a URL to extract page content.
2. Performs live keyword research using DuckDuckGo (native HTTP, no buggy libs).
3. Analyzes the content with Google Gemini for SEO improvements.

Author: AI Assistant
Version: 2.0 (Error-Proof Refactor)
==============================================================================
"""

import streamlit as st
import json
import requests
import re
from urllib.parse import urlparse, quote_plus
from datetime import datetime
from bs4 import BeautifulSoup
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ==============================================================================
# CONFIGURATION
# ==============================================================================

# Page configuration - MUST be first Streamlit command
st.set_page_config(
    page_title="Advanced SEO Evaluator AI",
    page_icon="üöÄ",
    layout="wide"
)

# System prompt for Gemini - embedded as per requirements
# Enhanced for richer, more detailed output with EXPLANATIONS
GEMINI_SYSTEM_PROMPT = """
You are an elite SEO specialist using modern ranking factors (EEAT, Helpfulness, Core Web Vitals).
You will receive page content and live keyword ideas from search results.

CURRENT DATE: {current_date}
Note: Use this date as context. Dates before this are in the past. Dates after this are in the future.

IMPORTANT: Assess the page ID/INTENT before scoring.
- **Navigational/Brand** (e.g., Apple, Google): Score on UX, clarity, and trust. Do NOT penalize for low text density.
- **Informational** (e.g., Blogs, Guides): Score on content depth, keyword coverage, and entity richness.
- **Transactional** (e.g., Product Pages): Score on trust signals, clear CTAs, and performance.

Your task:
1. Determine the Page Intent.
2. Pick the best Primary Keyword based on research context.
3. Analyze the page based on its Intent (not generic rules).
4. Provide exact replacements for Title, Meta, H1, etc., WITH REASONING.

You MUST output valid JSON matching this exact schema (no extra text):
{
    "page_intent": "<string: 'Navigational', 'Informational', 'Transactional'>",
    "seo_score": <integer 0-100>,
    "score_breakdown": {
        "keyword_optimization": <integer 0-25>,
        "content_quality": <integer 0-25>,
        "technical_seo": <integer 0-25>,
        "user_experience": <integer 0-25>
    },
    "critical_issues": [<list of max 5 specific critical issues>],
    "warnings": [<list of max 5 non-critical warnings>],
    "primary_keyword": "<string>",
    "primary_keyword_density": "<string>",
    "secondary_keywords": [<list of 7 keywords>],
    "long_tail_keywords": [<list of 5 long-tail phrases>],
    "suggested_title": {
        "text": "<string: max 60 chars>",
        "reasoning": "<string: why this title is better>"
    },
    "suggested_meta_description": {
        "text": "<string: max 160 chars>",
        "reasoning": "<string: why this description is better>"
    },
    "suggested_h1": {
        "text": "<string: recommended H1>",
        "reasoning": "<string: why this H1 is better>"
    },
    "suggested_h2s": [
        {"text": "<string>", "reasoning": "<string>"},
        {"text": "<string>", "reasoning": "<string>"}
    ],
    "improved_intro": {
        "text": "<string: rewritten paragraph>",
        "reasoning": "<string: why this is better>"
    },
    "improved_conclusion": {
        "text": "<string: suggested conclusion>",
        "reasoning": "<string: why this is better>"
    },
    "content_gaps": [<list of missing subtopics>],
    "internal_link_suggestions": [<list of 3 pages/topics to link to>],
    "word_count_analysis": {
        "current_estimate": <integer>,
        "recommended_minimum": <integer>,
        "verdict": "<string: 'Too short', 'Good', or 'Comprehensive'>"
    },
    "readability": {
        "level": "<string>",
        "suggestions": [<list of improvement tips>]
    },
    "eeat_analysis": {
        "expertise_signals": "<string>",
        "trust_signals": "<string>",
        "improvement_tips": [<list of suggestions>]
    },
    "action_items": [<list of top 5 prioritized actions>]
}
"""


# ==============================================================================
# CUSTOM CSS
# ==============================================================================

st.markdown("""
<style>
    /* Progress bar color */
    .stProgress > div > div > div > div {
        background-color: #00CC96;
    }
    /* Metric cards */
    .metric-card {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        margin-bottom: 20px;
    }
    /* Status boxes */
    .success-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
    }
    .warning-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #fff3cd;
        color: #856404;
        border: 1px solid #ffeeba;
    }
    /* Code blocks */
    .stCodeBlock {
        background-color: #1e1e1e !important;
    }
    /* Floating Chat Button */
    .floating-chat-container {
        position: fixed;
        bottom: 20px;
        right: 20px;
        z-index: 9999;
    }
    .chat-bubble {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 50%;
        width: 60px;
        height: 60px;
        font-size: 24px;
        cursor: pointer;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        transition: transform 0.2s, box-shadow 0.2s;
    }
    .chat-bubble:hover {
        transform: scale(1.1);
        box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
    }
    /* Chat Modal */
    .chat-modal {
        position: fixed;
        bottom: 90px;
        right: 20px;
        width: 380px;
        max-height: 500px;
        background: white;
        border-radius: 16px;
        box-shadow: 0 10px 40px rgba(0,0,0,0.2);
        z-index: 9998;
        overflow: hidden;
    }
    .chat-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 15px;
        font-weight: bold;
    }
    .chat-messages {
        height: 350px;
        overflow-y: auto;
        padding: 15px;
    }
    .chat-input-area {
        padding: 10px;
        border-top: 1px solid #eee;
    }
</style>
""", unsafe_allow_html=True)

# ==============================================================================
# LOGIN SYSTEM
# ==============================================================================

# Initialize session state for login
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False

# Get PIN from secrets (must be set in .streamlit/secrets.toml or Streamlit Cloud)
# Format in secrets.toml: ACCESS_PIN = "your-pin-here"
ACCESS_PIN = st.secrets.get("ACCESS_PIN", None)

# Login screen
if not st.session_state.authenticated:
    st.title("üîê SEO Evaluator - Login")
    st.markdown("Please enter the access code to continue.")
    
    # Check if PIN is configured
    if not ACCESS_PIN:
        st.error("‚ùå ACCESS_PIN not configured in secrets. Please add it to your secrets.toml")
        st.info("""
        **To fix (Streamlit Cloud):**
        1. Go to App Settings ‚Üí Secrets
        2. Add: `ACCESS_PIN = "your-4-digit-pin"`
        
        **To fix (Local):**
        1. Create `.streamlit/secrets.toml`
        2. Add: `ACCESS_PIN = "your-4-digit-pin"`
        """)
        st.stop()
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        pin_input = st.text_input(
            "Enter 4-digit PIN",
            type="password",
            max_chars=4,
            placeholder="****"
        )
        
        if st.button("üîì Login", use_container_width=True):
            if pin_input == ACCESS_PIN:
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå Incorrect PIN. Please try again.")
        
        st.caption("Contact the administrator if you don't have the access code.")
    
    st.stop()  # Stop execution until authenticated


# ==============================================================================
# SIDEBAR CONFIGURATION (Only shown after login)
# ==============================================================================

with st.sidebar:
    st.title("‚öôÔ∏è AI Configuration")
    
    # Provider Selection
    ai_provider = st.selectbox(
        "AI Provider",
        ["Google Gemini", "OpenAI"],
        index=0,
        help="Choose your AI provider"
    )
    st.session_state['ai_provider'] = ai_provider
    
    st.divider()
    
    # Model Selection based on provider
    if ai_provider == "Google Gemini":
        model_name = st.selectbox(
            "Select Model",
            [
                "gemini-3-flash-preview",
                "gemini-3-pro-preview",
                "gemini-2.5-pro",
            ],
            index=0,
            help="Choose the Gemini model"
        )
        
        # Model Guide - Gemini
        with st.expander("üìñ Model Guide"):
            st.markdown("""
            **üöÄ gemini-3-flash-preview** (Default)
            - Fast & cost-efficient ($0.075/1M input)
            
            **üíé gemini-3-pro-preview**
            - Most powerful ($1.25/1M input)
            
            **üî∑ gemini-2.5-pro**
            - Stable & reliable ($1.25/1M input)
            """)
    
    else:  # OpenAI
        model_name = st.selectbox(
            "Select Model",
            [
                "gpt-4o",
                "gpt-4o-mini",
                "gpt-5.2",
                "o3",
                "o4-mini",
            ],
            index=0,
            help="Choose the OpenAI model"
        )
        
        # Model Guide - OpenAI
        with st.expander("üìñ Model Guide"):
            st.markdown("""
            **üî∑ gpt-4o** (Default)
            - Multimodal, general use ($2.50/1M input)
            
            **‚ö° gpt-4o-mini**
            - Fast & cheap ($0.15/1M input)
            
            **üß† gpt-5.2**
            - Flagship reasoning ($5.00/1M input)
            
            **üî¨ o3**
            - Deep logic/STEM ($10.00/1M input)
            
            **üí° o4-mini**
            - Cost-effective reasoning ($2.00/1M input)
            """)
    
    st.session_state['model_name'] = model_name
    
    st.divider()
    
    # API Key Configuration
    st.subheader("üîë API Key")
    
    if ai_provider == "Google Gemini":
        use_custom_api = st.checkbox(
            "Use my own API key",
            value=False,
            help="Use your personal Google AI API key"
        )
        
        if use_custom_api:
            custom_api_key = st.text_input(
                "Enter Google AI API Key",
                type="password",
                placeholder="AIza...",
                help="Get from https://aistudio.google.com/app/apikey"
            )
            if custom_api_key and len(custom_api_key) > 10:
                api_key = custom_api_key
                st.success("‚úÖ Using your custom API key")
            else:
                st.warning("‚ö†Ô∏è Enter a valid API key")
                api_key = None
        else:
            if "GOOGLE_API_KEY" in st.secrets:
                api_key = st.secrets["GOOGLE_API_KEY"]
                if api_key and len(api_key) > 10:
                    st.success("‚úÖ Using default API key")
                else:
                    api_key = None
            else:
                st.warning("‚ö†Ô∏è No default key. Use your own.")
                api_key = None
    
    else:  # OpenAI
        openai_api_key = st.text_input(
            "Enter OpenAI API Key",
            type="password",
            placeholder="sk-...",
            help="Get from https://platform.openai.com/api-keys"
        )
        if openai_api_key and len(openai_api_key) > 10:
            api_key = openai_api_key
            st.success("‚úÖ OpenAI API key configured")
        else:
            st.warning("‚ö†Ô∏è Enter your OpenAI API key")
            api_key = None
    
    st.divider()
    
    st.info("""
    **How to use:**
    1. Select AI provider & model
    2. Enter a valid URL
    3. Review SEO audit & improvements
    """)
    
    st.divider()
    
    # API Usage Tracking
    st.subheader("üìä API Usage")
    
    # Initialize usage tracking
    if 'api_usage' not in st.session_state:
        st.session_state['api_usage'] = {'tokens': 0, 'cost': 0.0}
    
    # Model pricing (per 1M tokens)
    MODEL_PRICING = {
        # Gemini
        'gemini-3-flash-preview': {'input': 0.075, 'output': 0.30},
        'gemini-3-pro-preview': {'input': 1.25, 'output': 5.00},
        'gemini-2.5-pro': {'input': 1.25, 'output': 5.00},
        # OpenAI
        'gpt-4o': {'input': 2.50, 'output': 10.00},
        'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
        'gpt-5.2': {'input': 5.00, 'output': 20.00},
        'o3': {'input': 10.00, 'output': 40.00},
        'o4-mini': {'input': 2.00, 'output': 8.00},
    }
    st.session_state['model_pricing'] = MODEL_PRICING
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Tokens", f"{st.session_state['api_usage']['tokens']:,}")
    with col2:
        st.metric("Est. Cost", f"${st.session_state['api_usage']['cost']:.4f}")
    
    if st.button("üîÑ Reset Usage", use_container_width=True):
        st.session_state['api_usage'] = {'tokens': 0, 'cost': 0.0}
        st.rerun()
    
    st.divider()
    
    # Logout button
    if st.button("üö™ Logout", use_container_width=True):
        st.session_state.authenticated = False
        st.rerun()

# Check if we have a valid API key before proceeding
if not api_key:
    st.error("‚ùå No valid API key available. Please configure an API key in the sidebar.")
    st.stop()

# Store API key in session state for other pages
st.session_state['api_key'] = api_key


# ==============================================================================
# HELPER FUNCTIONS
# ==============================================================================

def validate_url(url: str) -> tuple[bool, str]:
    """
    Validates that the URL has a proper http/https scheme.
    Returns (is_valid, error_message).
    """
    if not url:
        return False, "URL cannot be empty."
    
    if not url.startswith(("http://", "https://")):
        return False, "URL must start with http:// or https://"
    
    try:
        parsed = urlparse(url)
        if not parsed.netloc:
            return False, "Invalid URL format - missing domain."
        return True, ""
    except Exception as e:
        return False, f"URL parsing error: {str(e)}"


@st.cache_data(ttl=3600, show_spinner=False)
def crawl_url(url: str) -> tuple[dict | None, str | None]:
    """
    Crawls the URL and extracts content using requests + BeautifulSoup.
    This is more reliable than WebBaseLoader for various sites.
    
    Returns: (data_dict, error_string)
    - data_dict contains: page_content, title, meta_description, h1
    - error_string is None on success, contains error message on failure
    """
    # NOTE: Removed 'br' (Brotli) from Accept-Encoding to avoid binary data issues
    # Only use gzip/deflate which requests handles automatically
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',  # NO brotli - causes binary issues
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        # Make request with timeout
        response = requests.get(url, headers=headers, timeout=20, allow_redirects=True)
        
        # Force encoding detection if not set
        if response.encoding is None or response.encoding == 'ISO-8859-1':
            # Try to detect from content-type header or use apparent_encoding
            response.encoding = response.apparent_encoding or 'utf-8'
        
        # Check for common error codes
        if response.status_code == 403:
            return None, "Access forbidden (403). The website is blocking automated requests."
        elif response.status_code == 404:
            return None, "Page not found (404). Please check the URL."
        elif response.status_code >= 400:
            return None, f"HTTP Error {response.status_code}. Unable to access the page."
        
        # Get text content with proper encoding
        html_content = response.text
        
        # Check if we got binary garbage
        if '\x00' in html_content[:1000] or not any(c.isalpha() for c in html_content[:500]):
            return None, "Received binary or corrupted data. The website may be blocking scrapers."
        
        # Parse HTML with html.parser (most reliable)
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract title BEFORE removing elements
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "No title found"
        
        # Extract meta description BEFORE removing elements
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc_tag:
            meta_desc_tag = soup.find('meta', attrs={'name': 'Description'})
        if not meta_desc_tag:
            meta_desc_tag = soup.find('meta', attrs={'property': 'og:description'})
        meta_description = meta_desc_tag.get('content', '') if meta_desc_tag else "No meta description found"
        
        # Extract H1 BEFORE removing elements
        h1_tag = soup.find('h1')
        h1 = h1_tag.get_text(strip=True) if h1_tag else "No H1 found"
        
        # Now remove non-content elements for text extraction
        for element in soup(['script', 'style', 'noscript', 'iframe', 'svg']):
            element.decompose()
        
        # Extract main text content - try multiple strategies
        page_content = ""
        
        # Strategy 1: Look for main content containers
        content_selectors = [
            soup.find('main'),
            soup.find('article'),
            soup.find('div', {'id': 'content'}),
            soup.find('div', {'id': 'main-content'}),
            soup.find('div', {'class': 'content'}),
            soup.find('div', {'class': 'main-content'}),
            soup.find('div', {'role': 'main'}),
        ]
        
        for container in content_selectors:
            if container:
                page_content = container.get_text(separator=' ', strip=True)
                if len(page_content) > 200:
                    break
        
        # Strategy 2: If no main content found, use body
        if len(page_content) < 200:
            body = soup.find('body')
            if body:
                # Remove nav/footer/header from body copy
                body_copy = BeautifulSoup(str(body), 'html.parser')
                for elem in body_copy(['nav', 'footer', 'header', 'aside', 'form']):
                    elem.decompose()
                page_content = body_copy.get_text(separator=' ', strip=True)
        
        # Strategy 3: Fallback to all text
        if len(page_content) < 100:
            page_content = soup.get_text(separator=' ', strip=True)
        
        # Clean up whitespace
        page_content = re.sub(r'\s+', ' ', page_content).strip()
        
        # Check for meaningful content
        if len(page_content) < 50:
            return None, "Page content is too short. The page may be mostly JavaScript-rendered."
        
        # Verify we have actual text content (not just symbols/numbers)
        alpha_count = sum(1 for c in page_content[:500] if c.isalpha())
        if alpha_count < 50:
            return None, "Page content appears to be non-text or corrupted."
        
        return {
            'page_content': page_content,
            'title': title,
            'meta_description': meta_description,
            'h1': h1,
            'url': url
        }, None
        
    except requests.exceptions.Timeout:
        return None, "Request timed out. The website took too long to respond."
    except requests.exceptions.SSLError:
        return None, "SSL certificate error. The website's security certificate is invalid."
    except requests.exceptions.ConnectionError:
        return None, "Connection error. Unable to reach the website."
    except Exception as e:
        return None, f"Unexpected error during crawl: {str(e)}"


@st.cache_data(ttl=3600, show_spinner=False)
def search_duckduckgo(query: str, max_results: int = 5) -> list[dict]:
    """
    Performs a DuckDuckGo search using multiple parsing strategies.
    Falls back gracefully if scraping fails.
    
    NOTE: This produces SERP-derived keyword ideas, not authoritative keyword volume data.
    """
    results = []
    
    try:
        # Use DuckDuckGo HTML search with proper headers
        search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': 'https://duckduckgo.com/',
        }
        
        response = requests.get(search_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Strategy 1: Try result__body class
            result_elements = soup.find_all('div', class_='result__body')[:max_results]
            
            # Strategy 2: Try result class if strategy 1 fails
            if not result_elements:
                result_elements = soup.find_all('div', class_='result')[:max_results]
            
            # Strategy 3: Try links-of-type pattern
            if not result_elements:
                result_elements = soup.find_all('div', class_='links_main')[:max_results]
            
            for elem in result_elements:
                # Try multiple title selectors
                title_elem = (
                    elem.find('a', class_='result__a') or 
                    elem.find('a', class_='result-link') or
                    elem.find('h2') or
                    elem.find('a')
                )
                
                # Try multiple snippet selectors  
                snippet_elem = (
                    elem.find('a', class_='result__snippet') or
                    elem.find('div', class_='result__snippet') or
                    elem.find('span', class_='result__snippet') or
                    elem.find('p')
                )
                
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if title_text and len(title_text) > 3:
                        results.append({
                            'title': title_text,
                            'link': title_elem.get('href', ''),
                            'snippet': snippet_elem.get_text(strip=True)[:200] if snippet_elem else ''
                        })
        
    except Exception as e:
        # Log but don't fail
        pass
    
    return results


def get_research_keywords(llm, page_content_snippet: str) -> tuple[str, str, list[dict]]:
    """
    Uses Gemini to identify the topic and generate keyword suggestions.
    Also attempts DuckDuckGo search for real-time SERP signals.
    
    If DuckDuckGo fails, Gemini will generate keywords based on the content alone.
    
    Returns: (topic_string, research_text, source_links)
    """
    # Step 1: Use Gemini to identify the main topic (3-6 words)
    topic_prompt = PromptTemplate.from_template(
        "Extract the main topic of this text in 3-6 words maximum. Only output the topic, nothing else.\n\nText: {text}"
    )
    topic_chain = topic_prompt | llm | StrOutputParser()
    
    try:
        topic = topic_chain.invoke({"text": page_content_snippet[:2000]}).strip()
        topic = topic.strip('"\'').strip()
    except Exception as e:
        topic = "general topic"
    
    # Step 2: Try DuckDuckGo searches
    search_queries = [
        f"{topic} keywords",
        f"{topic} best practices 2026",
        f"{topic} tips"
    ]
    
    all_results = []
    research_parts = []
    source_links = []
    
    for query in search_queries:
        results = search_duckduckgo(query, max_results=3)
        
        if results:
            research_parts.append(f"**Query:** {query}")
            for r in results:
                research_parts.append(f"- {r['title']}: {r['snippet'][:100]}...")
                source_links.append({'title': r['title'], 'link': r['link']})
            research_parts.append("")
            all_results.extend(results)
    
    # Step 3: If DuckDuckGo failed, use Gemini to generate keyword ideas
    if not all_results:
        keyword_prompt = PromptTemplate.from_template(
            """Based on this content about "{topic}", suggest 10 relevant SEO keywords.
            Format: One keyword per line, no numbering, no explanations.
            
            Content snippet: {content}"""
        )
        keyword_chain = keyword_prompt | llm | StrOutputParser()
        
        try:
            gemini_keywords = keyword_chain.invoke({
                "topic": topic,
                "content": page_content_snippet[:1500]
            })
            
            research_parts.append("**‚ö†Ô∏è Live Search Unavailable (Using AI-Generated Keywords)**")
            research_parts.append("*DuckDuckGo search connection failed (likely rate-limited). Using Gemini AI to imply keywords from content context instead.*")
            research_parts.append("")
            
            for kw in gemini_keywords.strip().split('\n')[:10]:
                kw = kw.strip().strip('-').strip('‚Ä¢').strip()
                if kw:
                    research_parts.append(f"- {kw}")
                    source_links.append({'title': kw, 'link': ''})
            research_parts.append("")
            research_parts.append("*Note: Keywords generated by AI based on page content.*")
        except:
            research_parts.append("**Note:** Could not fetch live keyword data.")
    
    research_text = "\n".join(research_parts) if research_parts else "No research data available."
    
    return topic, research_text, source_links



def analyze_content(llm, page_data: dict, research_data: str) -> str:
    """
    Main analysis function. Sends page content + research to Gemini for SEO analysis.
    
    Token usage guardrail: Content is truncated to avoid excessive token usage.
    """
    
    prompt = PromptTemplate.from_template(
        """
{system_prompt}

--- INPUT DATA ---

PAGE URL: {url}
PAGE TITLE: {title}
PAGE H1: {h1}
META DESCRIPTION: {meta_description}

LIVE KEYWORD RESEARCH FROM SERP:
{research}

PAGE CONTENT (First 15000 characters):
{content}

--- END INPUT ---

Analyze this page for SEO and output ONLY valid JSON matching the schema above.
"""
    )
    
    chain = prompt | llm | StrOutputParser()
    
    # Truncate content to manage token usage (Gemini has large context but we want speed)
    truncated_content = page_data['page_content'][:15000]
    
    # Inject current date into the system prompt
    current_date = datetime.now().strftime("%B %d, %Y")  # e.g., "January 19, 2026"
    formatted_system_prompt = GEMINI_SYSTEM_PROMPT.format(current_date=current_date)
    
    return chain.invoke({
        "system_prompt": formatted_system_prompt,
        "url": page_data.get('url', 'N/A'),
        "title": page_data.get('title', 'N/A'),
        "h1": page_data.get('h1', 'N/A'),
        "meta_description": page_data.get('meta_description', 'N/A'),
        "research": research_data,
        "content": truncated_content
    })


def parse_gemini_response(response_text: str) -> dict:
    """
    Safely parses the Gemini JSON response.
    Advanced cleaning to handle markdown fences, preambles, and verbose Pro model output.
    """
    try:
        # 1. Clean up known markdown wrapper patterns
        text = response_text.strip()
        
        # 2. Remove markdown code fences (```json ... ``` or ``` ... ```)
        text = re.sub(r'^```(?:json)?\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'```\s*$', '', text, flags=re.MULTILINE)
        text = text.strip()
        
        # 3. Use regex to find the largest outer JSON object
        # Pattern looks for { ... } across multiple lines
        json_match = re.search(r'\{[\s\S]*\}', text)
             
        if json_match:
            json_str = json_match.group(0)
            return json.loads(json_str)
        else:
            # If no JSON structure found, raise error
            raise ValueError("No JSON object found in response")
            
    except (json.JSONDecodeError, ValueError) as e:
        # If parsing fails, try to "fix" common issues
        try:
            # Sometimes models return single quotes instead of double
            fixed_text = text.replace("'", '"')
            # Also try to fix trailing commas (common LLM error)
            fixed_text = re.sub(r',\s*([\]\}])', r'\1', fixed_text)
            json_match = re.search(r'\{[\s\S]*\}', fixed_text)
            if json_match:
                return json.loads(json_match.group(0))
        except:
            pass
        raise e


# ==============================================================================
# MAIN APPLICATION
# ==============================================================================

st.title("üìä Advanced SEO Content Evaluator")
st.markdown("Enter a URL to audit its SEO performance and get AI-driven improvements based on live search data.")

# URL Input
url_input = st.text_input(
    "Enter URL to Analyze",
    placeholder="https://example.com/your-page",
    help="Enter the full URL including http:// or https://"
)

# Main analysis flow
if url_input:
    # Validate URL
    is_valid, error_msg = validate_url(url_input)
    
    if not is_valid:
        st.error(f"‚ùå {error_msg}")
        st.stop()
    
    # Initialize LLM based on provider
    try:
        ai_provider = st.session_state.get('ai_provider', 'Google Gemini')
        
        if ai_provider == "Google Gemini":
            chat_model = ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=api_key,
                temperature=0.3,
                max_output_tokens=4096
            )
        else:  # OpenAI
            chat_model = ChatOpenAI(
                model=model_name,
                api_key=api_key,
                temperature=0.3,
                max_tokens=4096
            )
    except Exception as e:
        st.error(f"‚ùå Failed to initialize {ai_provider} model: {str(e)}")
        st.info("Please verify your API key is valid and the model name is correct.")
        st.stop()
    
    # =========================================================================
    # PHASE 1: CRAWL
    # =========================================================================
    with st.status("üîç Phase 1: Crawling Website...", expanded=True) as status:
        page_data, crawl_error = crawl_url(url_input)
        
        if crawl_error:
            status.update(label="‚ùå Crawl Failed!", state="error", expanded=True)
            st.error(f"Error: {crawl_error}")
            st.stop()
        
        # Show extracted data summary
        st.write(f"**Title:** {page_data['title'][:80]}...")
        st.write(f"**H1:** {page_data['h1'][:80]}...")
        st.write(f"**Content Length:** {len(page_data['page_content']):,} characters")
        
        status.update(label="‚úÖ Website Crawled Successfully!", state="complete", expanded=False)
        
        # Save to session state
        st.session_state['page_data'] = page_data
    
    # =========================================================================
    # PHASE 2: RESEARCH
    # =========================================================================
    with st.status("üåê Phase 2: Live Keyword Research...", expanded=True) as status:
        topic, research_text, source_links = get_research_keywords(chat_model, page_data['page_content'])
        
        st.write(f"**Identified Topic:** {topic}")
        
        with st.expander("View Raw Search Data"):
            st.text(research_text)
        
        status.update(label="‚úÖ Research Completed!", state="complete", expanded=False)
        
        # Save to session state
        st.session_state['connected_topic'] = topic
        st.session_state['research_text'] = research_text
        st.session_state['source_links'] = source_links
    
    # =========================================================================
    # PHASE 3: GEMINI ANALYSIS
    # =========================================================================
    with st.spinner("ü§ñ Phase 3: Gemini is Analyzing SEO & Writing Improvements..."):
        try:
            analysis_response = analyze_content(chat_model, page_data, research_text)
            result = parse_gemini_response(analysis_response)
            
            # Save to session state
            st.session_state['analysis_result'] = result
            
        except json.JSONDecodeError as e:
            st.error("‚ùå Error parsing AI response. The model returned invalid JSON.")
            with st.expander("Debug: Raw AI Output"):
                st.code(analysis_response, language="text")
            st.info("Try clicking 'Analyze' again, or try a different Gemini model.")
            st.stop()
            
        except Exception as e:
            st.error(f"‚ùå Analysis error: {str(e)}")
            st.stop()
    
    # =========================================================================
    # PHASE 4: RESULTS DISPLAY (Enhanced for Reasoning)
    # =========================================================================
    st.divider()
    st.subheader("üìà Results & Recommendations")
    
    # Top Metrics Row
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        score = result.get('seo_score', 0)
        st.metric("SEO Score", f"{score}/100")
        st.progress(min(score / 100, 1.0))
        
    with col2:
        st.write("**Page Intent**")
        st.info(result.get('page_intent', 'General'))
        
    with col3:
        st.write("**Primary Keyword**")
        st.caption(result.get('primary_keyword', 'N/A'))
        
    with col4:
        word_count = result.get('word_count_analysis', {})
        st.write("**Word Count**")
        st.caption(f"{word_count.get('current_estimate', 'N/A')} words ({word_count.get('verdict', 'N/A')})")
    
    # Score Breakdown
    score_breakdown = result.get('score_breakdown', {})
    if score_breakdown:
        st.subheader("üìä Score Breakdown")
        sb_col1, sb_col2, sb_col3, sb_col4 = st.columns(4)
        with sb_col1:
            st.metric("Keywords", f"{score_breakdown.get('keyword_optimization', 0)}/25")
        with sb_col2:
            st.metric("Content", f"{score_breakdown.get('content_quality', 0)}/25")
        with sb_col3:
            st.metric("Technical", f"{score_breakdown.get('technical_seo', 0)}/25")
        with sb_col4:
            st.metric("UX", f"{score_breakdown.get('user_experience', 0)}/25")
    
    # Critical Issues & Warnings
    issues_col1, issues_col2 = st.columns(2)
    
    with issues_col1:
        st.subheader("üö® Critical Issues")
        critical_issues = result.get("critical_issues", [])
        if critical_issues:
            for issue in critical_issues:
                st.markdown(f"- üî¥ {issue}")
        else:
            st.success("‚úÖ No critical issues!")
            
    with issues_col2:
        st.subheader("‚ö†Ô∏è Warnings")
        warnings = result.get("warnings", [])
        if warnings:
            for warning in warnings:
                st.markdown(f"- üü† {warning}")
        else:
            st.success("‚úÖ No warnings!")
            
    # Priority Action Items
    st.subheader("üéØ Priority Action Items")
    action_items = result.get("action_items", [])
    if action_items:
        for i, action in enumerate(action_items, 1):
            st.markdown(f"**{i}.** {action}")
    else:
        st.write("No action items generated.")

    st.divider()
    
    st.header("‚ú® Targeted Improvements")
    
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "üè∑Ô∏è Meta Tags", 
        "üìù Content", 
        "üîë Keywords",
        "üèÜ EEAT & Trust", 
        "üìñ Readability", 
        "üîó Sources"
    ])
    
    with tab1:
        st.subheader("Title & Meta Description")
        
        # Display with Reasoning
        s_title = result.get('suggested_title', {})
        st.write("**Suggested Title:**")
        if isinstance(s_title, dict):
            st.code(s_title.get('text', 'N/A'), language='html')
            if s_title.get('reasoning'):
                st.caption(f"üí° *Why: {s_title.get('reasoning')}*")
        else:
            st.code(s_title, language='html')
            
        s_meta = result.get('suggested_meta_description', {})
        st.write("**Suggested Meta Description:**")
        if isinstance(s_meta, dict):
            st.code(s_meta.get('text', 'N/A'), language='html')
            if s_meta.get('reasoning'):
                st.caption(f"üí° *Why: {s_meta.get('reasoning')}*")
        else:
            st.code(s_meta, language='html')
            
        s_h1 = result.get('suggested_h1', {})
        st.write("**Suggested H1:**")
        if isinstance(s_h1, dict):
            st.code(s_h1.get('text', 'N/A'), language='html')
            if s_h1.get('reasoning'):
                st.caption(f"üí° *Why: {s_h1.get('reasoning')}*")
        else:
            st.code(s_h1, language='html')
            
        st.subheader("Suggested H2s")
        for h2 in result.get('suggested_h2s', []):
            if isinstance(h2, dict):
                st.markdown(f"- **{h2.get('text')}**")
                st.caption(f"  ‚îî‚îÄ *{h2.get('reasoning')}*")
            else:
                st.markdown(f"- {h2}")

    with tab2:
        st.subheader("Content Optimization")
        
        s_intro = result.get('improved_intro', {})
        with st.expander("üìÑ Rewritten Introduction (Optimized)", expanded=True):
            if isinstance(s_intro, dict):
                st.markdown(s_intro.get('text', 'N/A'))
                if s_intro.get('reasoning'):
                    st.caption(f"üí° *Why: {s_intro.get('reasoning')}*")
            else:
                st.markdown(s_intro)
                
        s_conc = result.get('improved_conclusion', {})
        with st.expander("üîö Suggested Conclusion"):
            if isinstance(s_conc, dict):
                st.markdown(s_conc.get('text', 'N/A'))
                if s_conc.get('reasoning'):
                    st.caption(f"üí° *Why: {s_conc.get('reasoning')}*")
            else:
                st.markdown(s_conc)
                
        st.subheader("Content Gaps")
        for gap in result.get('content_gaps', []):
            st.markdown(f"- üìå {gap}")
            
        st.subheader("Internal Linking Opportunities")
        for link in result.get('internal_link_suggestions', []):
            st.markdown(f"- üîó {link}")

    with tab3:
        st.subheader("Keyword Strategy")
        st.table({
            "Metric": ["Primary Keyword", "Density"],
            "Value": [result.get('primary_keyword'), result.get('primary_keyword_density')]
        })
        
        col_k1, col_k2 = st.columns(2)
        with col_k1:
            st.write("**Secondary Keywords**")
            for kw in result.get('secondary_keywords', []):
                st.markdown(f"- {kw}")
        with col_k2:
            st.write("**Long-Tail Opportunities**")
            for kw in result.get('long_tail_keywords', []):
                st.markdown(f"- {kw}")

    
    with tab4:
        eeat = result.get('eeat_analysis', {})
        if eeat:
            st.markdown("### Expertise Signals")
            st.write(eeat.get('expertise_signals', 'N/A'))
            
            st.markdown("### Trust Signals")
            st.write(eeat.get('trust_signals', 'N/A'))
            
            st.markdown("### EEAT Improvement Tips")
            for tip in eeat.get('improvement_tips', []):
                st.markdown(f"- üí° {tip}")
        else:
            st.write("EEAT analysis not available.")
    
    with tab5:
        readability = result.get('readability', {})
        if readability:
            st.markdown(f"### Readability Level: **{readability.get('level', 'N/A')}**")
            
            st.markdown("### Improvement Suggestions")
            for suggestion in readability.get('suggestions', []):
                st.markdown(f"- ‚úèÔ∏è {suggestion}")
        
        word_count = result.get('word_count_analysis', {})
        if word_count:
            st.markdown("### Word Count Analysis")
            st.write(f"**Current:** ~{word_count.get('current_estimate', 'N/A')} words")
            st.write(f"**Recommended Minimum:** {word_count.get('recommended_minimum', 'N/A')} words")
            st.write(f"**Verdict:** {word_count.get('verdict', 'N/A')}")
    
    with tab6:
        st.markdown("### Research Source Links")
        st.caption("These links were used to derive keyword ideas:")
        if source_links:
            for link in source_links[:10]:
                if link.get('link'):
                    st.markdown(f"- [{link['title'][:50]}...]({link['link']})")
        else:
            st.write("No source links available.")
    
    # PageSpeed Reminder
    st.divider()
    st.info("üì± **Reminder:** Test your page's mobile speed at [PageSpeed Insights](https://pagespeed.web.dev/)")
    
    # Debug Section
    with st.expander("üîß Debug: Raw JSON Response"):
        st.json(result)

# ==============================================================================
# FLOATING CHAT ASSISTANT
# ==============================================================================

# Initialize chat state
if 'chat_messages' not in st.session_state:
    st.session_state.chat_messages = []

# Create floating chat button using Streamlit popover
with st.popover("üí¨ Ask AI", use_container_width=False):
    st.markdown("### ü§ñ SEO Chat Assistant")
    
    # Check if we have analysis context
    if 'analysis_result' in st.session_state and 'page_data' in st.session_state:
        st.success("‚úÖ Analysis context loaded")
    else:
        st.warning("‚ö†Ô∏è Run an analysis first for context-aware chat")
    
    st.divider()
    
    # Quick actions
    st.markdown("**Quick Actions:**")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üìù Rewrite Intro", use_container_width=True):
            st.session_state.chat_messages.append({"role": "user", "content": "Rewrite the introduction with better SEO"})
    with col2:
        if st.button("‚ùì Generate FAQs", use_container_width=True):
            st.session_state.chat_messages.append({"role": "user", "content": "Generate 5 FAQ questions"})
    
    st.divider()
    
    # Chat messages
    chat_container = st.container(height=200)
    with chat_container:
        for msg in st.session_state.chat_messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
    
    # Chat input
    if prompt := st.chat_input("Ask about your SEO..."):
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        
        if 'analysis_result' in st.session_state and api_key:
            try:
                ai_prov = st.session_state.get('ai_provider', 'Google Gemini')
                if ai_prov == "Google Gemini":
                    chat_llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key, temperature=0.4)
                else:
                    chat_llm = ChatOpenAI(model=model_name, api_key=api_key, temperature=0.4)
                
                res = st.session_state['analysis_result']
                ctx = f"SEO Score: {res.get('seo_score')}/100. Issues: {res.get('critical_issues')}. Question: {prompt}"
                response = chat_llm.invoke(ctx).content
                st.session_state.chat_messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.session_state.chat_messages.append({"role": "assistant", "content": f"Error: {e}"})
        else:
            st.session_state.chat_messages.append({"role": "assistant", "content": "Run an analysis first!"})
        st.rerun()
    
    st.divider()
    st.page_link("pages/1_üí¨_Ask_AI.py", label="üî≤ Full Screen Chat", icon="üí¨")

